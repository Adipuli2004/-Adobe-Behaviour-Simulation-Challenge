{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7176600,"sourceType":"datasetVersion","datasetId":4147300},{"sourceId":7184308,"sourceType":"datasetVersion","datasetId":4153031},{"sourceId":7186775,"sourceType":"datasetVersion","datasetId":4154868},{"sourceId":7186806,"sourceType":"datasetVersion","datasetId":4154892},{"sourceId":7192726,"sourceType":"datasetVersion","datasetId":4159341}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing all the dependencies ","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install av\n!pip install pandas\n!pip install transformers\n!pip install re\n!pip install ast\n!pip install requests\n!pip install numpy\n!pip install torc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing all the necessary libraries and functions","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport transformers\nimport av\nimport re\nimport ast\nimport requests\nimport numpy as np\nimport torch\nfrom transformers import AutoImageProcessor, AutoTokenizer, VisionEncoderDecoderModel,AutoModel ,BitsAndBytesConfig, AutoModelForCausalLM\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport requests","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import the test dataset and company-to-sector mapping\nEnter path of the test dataset and company-to-sector mapping excel files","metadata":{}},{"cell_type":"code","source":"ds = pd.read_excel('/kaggle/input/test-ds/test_data.xlsx')\ncompany2sector = pd.read_excel('/kaggle/input/company2sector/company2sector (1).xlsx')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load all the models ,tokenizers and image processors","metadata":{}},{"cell_type":"code","source":"processor_1 = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\ntokenizer_vid = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel_vid = VisionEncoderDecoderModel.from_pretrained(\"Neleac/timesformer-gpt2-video-captioning\").to(device)\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\nmodel_img = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\nmodel_final = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", quantization_config=bnb_config, device_map=\"auto\")\ntokenizer_final = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions to generate captions ,prompts and final tweet","metadata":{}},{"cell_type":"code","source":"count = 0\ncount1 = 0\n\ndef video_caption(video_url):\n    try:\n\n        urls_with_bitrate = re.findall(r\"url='(.*?)', bitrate=(\\d+|None)\", video_url)\n\n\n        filtered_urls = [url for url, bitrate in urls_with_bitrate if bitrate.lower() != 'none']\n        final_url = filtered_urls[0]\n        with requests.get(final_url, stream=True) as response:\n            response.raise_for_status()\n            with open(\"downloaded_video.mp4\", \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n\n        container = av.open(\"downloaded_video.mp4\")\n        seg_len = container.streams.video[0].frames\n        clip_len = model_vid.config.encoder.num_frames\n        indices = set(np.linspace(0, seg_len, num=clip_len, endpoint=False).astype(np.int64))\n        frames = []\n        container.seek(0)\n\n        for i, frame in enumerate(container.decode(video=0)):\n            if i in indices:\n                frames.append(frame.to_ndarray(format=\"rgb24\"))\n        gen_kwargs = {\n            \"min_length\": 10,\n            \"max_length\": 30,\n            \"num_beams\": 4,\n        }\n        pixel_values = processor_1(frames, return_tensors=\"pt\").pixel_values.to(device)\n        tokens = model_vid.generate(pixel_values, **gen_kwargs)\n        return tokenizer_vid.batch_decode(tokens, skip_special_tokens=True)[0]\n    except Exception as e:\n        print(e)\n        return \"blank\"\n    \ndef img_caption(img_url):\n    try:\n        image_data_ast = ast.parse(img_url)\n        last_url = None\n\n        for node in ast.walk(image_data_ast):\n            if isinstance(node, ast.Str) and \"https://\" in node.s:\n                last_url = node.s\n        raw_image = Image.open(requests.get(last_url, stream=True).raw).convert('RGB')\n        inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n        return processor.decode(model_img.generate(**inputs)[0], skip_special_tokens=True)\n    except Exception as e:\n        print(e)\n        return \"blank\"\n\n\ndef captions(url):\n    global count\n    count+=1\n    if (count%500 == 499):\n        print(count)\n    if url[1] == 'V' or url[1] == 'G':\n        return video_caption(url)\n    else:\n        return img_caption(url)\n        \ndef get_sector(company):\n    return company2sector[company2sector['Company']== company]['Sector'].values[0]\n\ndef get_prompt(row):\n    if row['caption'] != 'blank':\n        result=(\n            \"Company : \" + row['inferred company'] + \"\\n\" +\n            \"Sector : \" + row['sector'] + \"\\n\" +\n            \"Likes : \" + str(row['likes']) + \"\\n\" +\n            \"Image/Video Description : \" + row['caption'] + \"\\n\" +\n            \"Goal: Generate a tweet text to increase brand awareness and to maximize the likes.The output only the tweet text.\")\n    else:\n        result=(\n            \"Company : \" + row['inferred company'] + \"\\n\" +\n            \"Sector : \" + row['sector'] + \"\\n\" +\n            \"Goal: Generate a tweet text to increase brand awareness and to maximize the likes.The output should consist only of tweet text \")\n    return result\n\ndef gen_tweet(prompt):\n    global count1\n    count1+=1\n    messages = [\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n\n\n    encodeds = tokenizer_final.apply_chat_template(messages, return_tensors=\"pt\")\n\n    model_inputs = encodeds.to(device)\n\n\n    generated_ids = model_final.generate(model_inputs, max_new_tokens=200, do_sample=True ,pad_token_id= tokenizer_final.eos_token_id)\n    decoded = tokenizer_final.batch_decode(generated_ids)\n    if count1%500 == 499:\n        print(count1)\n    return re.findall(r'\\[/INST\\](.*?)<\\/s>', decoded[0])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating captions for the Video/Image/GIF","metadata":{}},{"cell_type":"code","source":"ds.loc[:, 'caption'] = ds['media'].apply(captions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Writing the sectors in which the company works in the dataframe","metadata":{}},{"cell_type":"code","source":"ds.loc[:, 'sector'] = ds['inferred company'].apply(get_sector)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating prompts for the LLM to generate tweet","metadata":{}},{"cell_type":"code","source":"ds['prompt'] = ds.apply(get_prompt, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating outputs from the prompts","metadata":{}},{"cell_type":"code","source":"ds.loc[:, 'output'] = ds['prompt'].apply(gen_tweet)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving the final dataframe in the form of excel","metadata":{}},{"cell_type":"code","source":"ds.to_excel('submission2.xlsx')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}