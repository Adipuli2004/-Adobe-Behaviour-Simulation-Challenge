{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7187190,"sourceType":"datasetVersion","datasetId":4155170},{"sourceId":7187200,"sourceType":"datasetVersion","datasetId":4155177},{"sourceId":7190050,"sourceType":"datasetVersion","datasetId":4157275}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import all the necessary libraries","metadata":{}},{"cell_type":"code","source":"import re\nimport requests\nfrom io import BytesIO\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader,random_split\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom transformers import ViTModel,ViTImageProcessor\nfrom PIL import Image \nimport torchvision.transforms as transforms ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-12-13T08:02:22.672944Z","iopub.execute_input":"2023-12-13T08:02:22.673366Z","iopub.status.idle":"2023-12-13T08:02:22.680978Z","shell.execute_reply.started":"2023-12-13T08:02:22.673331Z","shell.execute_reply":"2023-12-13T08:02:22.679479Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Class initialization of model architecture","metadata":{}},{"cell_type":"code","source":"class transformermodel(nn.Module):\n    def __init__(self):\n        super(transformermodel, self).__init__()\n        self.base_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n        self.base_model_1 = AutoModel.from_pretrained('vinai/bertweet-base')\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n        for param in self.base_model_1.parameters():\n            param.requires_grad = False\n        self.img_enc1 = nn.TransformerEncoderLayer(768, 8, activation='gelu', batch_first=True)\n        self.txt_enc1 = nn.TransformerEncoderLayer(768, 8, activation='gelu', batch_first=True)\n        self.txt_enc2 = nn.TransformerEncoderLayer(768, 12, activation='gelu', batch_first=True)\n        self.img_enc2 = nn.TransformerEncoderLayer(768, 12, activation='gelu', batch_first=True)\n        self.txt_enc3 = nn.TransformerEncoderLayer(768, 12, activation='gelu', batch_first=True)\n        self.img_enc3 = nn.TransformerEncoderLayer(768, 12, activation='gelu', batch_first=True)\n        self.img_dec1 = nn.TransformerDecoderLayer(768, 12, activation='gelu', batch_first=True)\n        self.txt_dec1 = nn.TransformerDecoderLayer(768, 12, activation='gelu', batch_first=True)\n        self.txt_dec2 = nn.TransformerDecoderLayer(768, 8, activation='gelu', batch_first=True)\n        self.img_dec2 = nn.TransformerDecoderLayer(768, 8, activation='gelu', batch_first=True)\n        self.img_dec3 = nn.TransformerDecoderLayer(768, 12, activation='gelu', batch_first=True)\n        self.txt_dec3 = nn.TransformerDecoderLayer(768, 12, activation='gelu', batch_first=True)\n\n        self.fc = nn.Linear(768, 768)\n        self.fc1 = nn.Linear(768, 768)\n        self.fc2 = nn.Linear(768, 768)\n        self.fc3 = nn.Linear(768, 768)\n        self.corr1 = nn.Linear(768, 256)\n        self.corr1_1 = nn.Linear(256, 64)\n        self.corr2 = nn.Linear(768, 256)\n        self.corr2_1 = nn.Linear(256, 64)\n        self.img = nn.Linear(768, 256)\n        self.img_1 = nn.Linear(256, 64)\n        self.txt = nn.Linear(768, 256)\n        self.txt_1 = nn.Linear(256, 64)\n        self.FC = nn.Linear(64 * 4 + 5, 128)\n        self.FC1 = nn.Linear(128, 64)\n        self.FC2 = nn.Linear(64, 1)\n\n    def forward(self, txt, mask, img, followers,time, is_photo, is_gif, is_video):\n        txt_output = self.base_model_1(txt, attention_mask=mask)\n        img_output = self.base_model(img)\n        img_output['last_hidden_state'] = (self.img_enc1(img_output['last_hidden_state']))\n        txt_output['last_hidden_state'] = (self.txt_enc1(txt_output['last_hidden_state']))\n        cross_attention_img = self.img_dec1(img_output['last_hidden_state'], txt_output['last_hidden_state'])\n        cross_attention_txt = self.txt_dec1(txt_output['last_hidden_state'], img_output['last_hidden_state'])\n        cross_attention_img = (self.img_enc2(cross_attention_img))\n        cross_attention_txt = (self.txt_enc2(cross_attention_txt))\n        cross_attention_img1 = ((self.img_dec2(cross_attention_img, txt_output['last_hidden_state'])))\n        cross_attention_txt1 = ((self.txt_dec2(cross_attention_txt, img_output['last_hidden_state'])))\n        cross_attention_img1 = (self.img_enc3(cross_attention_img1))\n        cross_attention_txt1 = (self.txt_enc3(cross_attention_txt1))\n        cross_attention_img2 = (self.img_dec3(cross_attention_img1, txt_output['last_hidden_state']))\n        cross_attention_txt2 = (self.txt_dec3(cross_attention_txt1, img_output['last_hidden_state']))\n        corr1 = F.gelu(self.fc(cross_attention_img2[:, 0, :]))\n        corr2 = F.gelu(self.fc1(cross_attention_txt2[:, 0, :]))\n        corr1 = F.gelu((self.corr1_1(F.elu((self.corr1(corr1))))))\n        corr2 = F.gelu((self.corr2_1(F.elu((self.corr2(corr2))))))\n        img_out = img_output['pooler_output']\n        txt_out = txt_output['pooler_output']\n        img_out = F.gelu((self.img_1(F.elu((self.img(img_out))))))\n        txt_out = F.gelu((self.txt_1(F.elu((self.txt(txt_out))))))\n        pooler_output = (torch.cat([corr1, corr2, img_out, txt_out, followers, is_photo, is_gif, is_video, time], dim=1))\n        pooler_output = (F.gelu((self.FC1(F.leaky_relu((self.FC(pooler_output)))))))\n        return torch.squeeze(self.FC2(pooler_output))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T08:03:18.738172Z","iopub.execute_input":"2023-12-13T08:03:18.738601Z","iopub.status.idle":"2023-12-13T08:03:18.758518Z","shell.execute_reply.started":"2023-12-13T08:03:18.738568Z","shell.execute_reply":"2023-12-13T08:03:18.756777Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Load the model\nReplace '/path/to/your/models/model.pth' with the actual path for your model file.","metadata":{}},{"cell_type":"code","source":"model = torch.load('/kaggle/input/final-model/final.pth', map_location=device)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T08:03:19.100459Z","iopub.execute_input":"2023-12-13T08:03:19.100859Z","iopub.status.idle":"2023-12-13T08:03:28.172529Z","shell.execute_reply.started":"2023-12-13T08:03:19.100827Z","shell.execute_reply":"2023-12-13T08:03:28.170878Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Function to remove emojis or others special characters excluding punctuation marks from the tweet text\nThe Bertweet model tokenizer is not able to tokenize special characters","metadata":{}},{"cell_type":"code","source":"def preprocess(text):\n    # Remove emojis\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n                               u\"\\U000024C2-\"  # Exclude '#' from removal\n                               u\"\\U0001F251\"  # Exclude '#' from removal\n                               \"]+\", flags=re.UNICODE)\n    text_no_emojis = emoji_pattern.sub(r'', text)\n\n    # Remove special characters, keeping digits, '#' and basic punctuation marks\n    text_no_special_chars = re.sub(r'[^\\w\\s\\d#.,!?;:()\\'\"]', '', text_no_emojis)\n\n    # Split the text into words\n    words = text_no_special_chars.split()\n\n    # Concatenate words into a single string with space-separated words\n    concatenated_string = ' '.join(words)\n\n    return concatenated_string\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader class","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, annotations_file):\n        ds = pd.read_excel(annotations_file)\n        self.dates = pd.to_datetime(ds['date'])\n        curr = pd.Timestamp('2023-12-05 12:00:00')\n        self.pattern =  r'https[^\\' ]*\\''\n        self.dates = ((curr - self.dates).dt.total_seconds() / (24 * 3600))\n        self.texts = [preprocess(text) for text in ds['content']]\n        self.followers = torch.tensor(ds['followers'], dtype=torch.float)\n        self.labels = torch.tensor(ds['likes'], dtype=torch.float)\n        self.media = ds['media']\n        self.id = ds['id']\n        self.is_photo = torch.tensor(ds['media'].apply(lambda x: 1 if len(x) >= 2 and x[1] == 'P' else 0), dtype=torch.float)\n        self.is_video = torch.tensor(ds['media'].apply(lambda x: 1 if len(x) >= 2 and x[1] == 'V' else 0), dtype=torch.float)\n        self.is_gif = torch.tensor(ds['media'].apply(lambda x: 1 if len(x) >= 2 and x[1] == 'G' else 0), dtype=torch.float)\n        self.img_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n        self.txt_tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n\n        # Tokenize all texts during initialization\n        self.tokenized_texts = self.txt_tokenizer(self.texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n\n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, index):\n        followers = self.followers[index]\n        label = self.labels[index]\n        is_photo = self.is_photo[index]\n        is_video = self.is_video[index]\n        is_gif = self.is_gif[index]\n        url = re.search(self.pattern, self.media[index])\n        try:\n            response = requests.get(url[0][0:-1])\n            if response.status_code == 200:\n                image = Image.open(BytesIO(response.content))\n                if image.mode == 'RGBA':\n                    image = image.convert('RGB')\n                img_tensor = torch.tensor(self.img_processor(image)['pixel_values'][0], dtype=torch.float)\n            else:\n                img_tensor = torch.zeros(3, 224, 224)\n        except Exception as e:\n            img_tensor = torch.zeros(3, 224, 224)\n        time = torch.tensor(self.dates[index], dtype=torch.float)\n\n        # Access input_ids and attention_mask from tokenized_texts\n        input_ids = self.tokenized_texts['input_ids'][index]\n        attention_mask = self.tokenized_texts['attention_mask'][index]\n\n        sample = {\n            \"Text\": {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n            },\n            \"followers\": torch.unsqueeze(followers,0),\n            \"img\": img_tensor,\n            \"likes\": label,\n            \"time\": torch.unsqueeze(time,0),\n            \"is_photo\" : torch.unsqueeze(is_photo,0),\n            \"is_video\" : torch.unsqueeze(is_video,0),\n            \"is_gif\" : torch.unsqueeze(is_gif,0),\n            \"id\" : self.id[index],\n        }\n\n        return sample","metadata":{"execution":{"iopub.status.busy":"2023-12-13T08:03:31.807451Z","iopub.execute_input":"2023-12-13T08:03:31.807844Z","iopub.status.idle":"2023-12-13T08:03:31.826677Z","shell.execute_reply.started":"2023-12-13T08:03:31.807803Z","shell.execute_reply":"2023-12-13T08:03:31.825423Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Creating an instance of the dataset\nReplace '/kaggle/input/dataset/PS_8_dataset_with_followers.xlsx' with the actual path for your dataset excel file.","metadata":{}},{"cell_type":"code","source":"ds = CustomDataset('/kaggle/input/dataset/PS_8_dataset_with_followers.xlsx')  # this is a torch dataset instance\ndf = pd.read_excel('/kaggle/input/dataset/PS_8_dataset_with_followers.xlsx')  # this is a pandas dataframe","metadata":{"execution":{"iopub.status.busy":"2023-12-13T08:03:32.174671Z","iopub.execute_input":"2023-12-13T08:03:32.175082Z","iopub.status.idle":"2023-12-13T08:06:02.175211Z","shell.execute_reply.started":"2023-12-13T08:03:32.175048Z","shell.execute_reply":"2023-12-13T08:06:02.174127Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3213385a7a14c92bc997764814eba02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e7b97034c6d4c9c96b425b3097b0314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dca9ab1e1a547f2b464147e84428de9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b36b5c2766364808a41b47c64d8742f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fed8de8a59b74f08b03f820ce29b1d8c"}},"metadata":{}}]},{"cell_type":"code","source":"# initializing the dataloader \ndataloader = DataLoader(ds, batch_size=1, shuffle=False,num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T08:06:30.834539Z","iopub.execute_input":"2023-12-13T08:06:30.834950Z","iopub.status.idle":"2023-12-13T08:06:30.839510Z","shell.execute_reply.started":"2023-12-13T08:06:30.834916Z","shell.execute_reply":"2023-12-13T08:06:30.838735Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Predicting likes for the tweets","metadata":{}},{"cell_type":"code","source":"outputs = []  # all the outputs will be stored in this list \nmodel.eval()\nc = 0\nwith torch.no_grad():\n    for data in dataloader:\n        c+=1\n        try:\n            text,mask,imgs,followers,time,is_photo,is_gif,is_video,labels = data['Text']['input_ids'],data['Text']['attention_mask'],data['img'],data['followers'],data['time'],data['is_photo'],data['is_gif'],data['is_video'],data['likes']\n            text,mask,imgs,followers,time,is_photo,is_gif,is_video,labels = text.to(device),mask.to(device),imgs.to(device),followers.to(device),time.to(device),is_photo.to(device),is_gif.to(device),is_video.to(device),labels.to(device)\n\n            output = model(text,mask,imgs,followers,time,is_photo,is_gif,is_video)\n            outputs.append(output.item())\n        except Exception as e:\n            print(data['id'])\n            print(e)\n            break\n        if c==1000:\n            print(c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding a new column 'outputs' in the dataframe","metadata":{}},{"cell_type":"code","source":"df['outputs'] = outputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the dataframe in the form of excel file","metadata":{}},{"cell_type":"code","source":"df.to_excel('submission1.xlsx')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T08:10:19.177058Z","iopub.execute_input":"2023-12-13T08:10:19.177666Z","iopub.status.idle":"2023-12-13T08:10:19.217432Z","shell.execute_reply.started":"2023-12-13T08:10:19.177606Z","shell.execute_reply":"2023-12-13T08:10:19.216110Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}